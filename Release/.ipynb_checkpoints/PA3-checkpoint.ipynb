{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Introduction and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project guides you through the general pipeline used to build and train a convolutional neural network (CNN) for an image classification task. You'll be implementing data augmentation, improving upon a baseline model, and trying to fool your CNN with adversarial images.\n",
    "\n",
    "\n",
    "We will be using a library called PyTorch which simplifies many of the low-level implementation details of neural networks for us, so that we can focus on the high-level deep learning concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New to Jupyter notebooks?\n",
    "Check out this [quick video introduction](https://www.youtube.com/watch?v=jZ952vChhuI) to Jupyter notebooks before starting, and feel free to post on Piazza if you run into any issues.\n",
    "\n",
    "### Using this notebook\n",
    "This notebook will walk you through the different parts of the assignment, with detailed instructions and explanations at every step. You'll see red <font color=\"red\">TODO [code]</font> for things you need to write in `student.py`, and red <font color=\"red\">TODO [writeup]</font> for things you need to answer in your writeup. Please write the answers for <font color=\"red\">TODO [writeup]</font> in the same document as the questions for image formation, camera calibration, and stereo.\n",
    "\n",
    "\n",
    "### Your code\n",
    "Once again, **all of your code needs to be written in `student.py`, and not in this notebook!** We will not grade any code written in this notebook (since you will not be submitting it); this notebook is meant to serve as a central tool that you can use to run your code and visualize your outputs. It also contains detailed explanations at every step to guide you. If anything is unclear, please post on Piazza!\n",
    "\n",
    "\n",
    "### Python version\n",
    "The top-right of this notebook should display a Python version; please make sure that it says Python 3 before proceeding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%matplotlib inline\n",
    "%aimport student\n",
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from skimage import io\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "\n",
    "# Constants\n",
    "classes = [\n",
    "     'bighorn-sheep',\n",
    "     'bison',\n",
    "     'black-stork',\n",
    "     'brown-bear',\n",
    "     'bullfrog',\n",
    "     'camel',\n",
    "     'gazelle',\n",
    "     'golden-retriever',\n",
    "     'goldfish',\n",
    "     'ladybug',\n",
    "     'lion',\n",
    "     'orangutan',\n",
    "     'penguin',\n",
    "     'persian-cat',\n",
    "     'pig',\n",
    "     'puma'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Unpacking the dataset\n",
    "\n",
    "Unzip the `data.zip` file. You should now have a folder called `data` with this structure:\n",
    "\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        bighorn-sheep/\n",
    "            XXXX.JPEG\n",
    "        goldfish/\n",
    "            XXXX.JPEG\n",
    "        ...\n",
    "    val/\n",
    "        bighorn-sheep/\n",
    "            XXXX.JPEG\n",
    "        goldfish/\n",
    "            XXXX.JPEG\n",
    "        ...\n",
    "```\n",
    "\n",
    "The names of the actual images (`XXXX.JPEG` in the diagram above) don't matter - only the folder structure matters, where the names of the folders under `train/` and `val/` correspond to the class names.\n",
    "\n",
    "**Data summary:**\n",
    "There are 16 classes, each with 500 training images and 50 validation images. Each image is 64x64 with 3 channels.\n",
    "\n",
    "The classes are the following:\n",
    "```\n",
    "bighorn-sheep\n",
    "bison\n",
    "black-stork\n",
    "brown-bear\n",
    "bullfrog\n",
    "camel\n",
    "gazelle\n",
    "golden-retriever\n",
    "goldfish\n",
    "ladybug\n",
    "lion\n",
    "orangutan\n",
    "penguin\n",
    "persian-cat\n",
    "pig\n",
    "puma\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 2: A quick sanity check...\n",
    "Before training any machine learning model, it's important to fully understand the data that is being dealt with. What does it look like, and does it match our expectations? In this part, we'll run code that looks into the dataset and shows us what it contains.\n",
    "\n",
    "To help with this, we have first defined a `show_image` function that takes an array as input and visualizes it as an image (as long as it is shaped like an image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(data):\n",
    "    '''\n",
    "    Given an image-like array, plot it as an image.\n",
    "    \n",
    "    Parameters:\n",
    "        data: a numpy array shaped like an image (could be RGB or grayscale)\n",
    "    '''\n",
    "    plt.imshow(data, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_N_images(N, class_name, root_folder):\n",
    "    \"\"\"\n",
    "    Returns N images from [class_string] class under [root_folder]\n",
    "    \n",
    "    Inputs:\n",
    "        N             scalar; number of images to return\n",
    "        \n",
    "        class_name    string; name of class to get images from.\n",
    "                      Must be name of a folder under root_folder\n",
    "                      where images of the class are held.\n",
    "                      \n",
    "        root_folder   string; relative path to folder containing subfolders\n",
    "                      with image classes.\n",
    "                      \n",
    "                      Example structure:\n",
    "                      root_folder/\n",
    "                          class_x/\n",
    "                              XXX.JPEG\n",
    "    Returns:\n",
    "        images        length N list of numpy arrays of images of class [class_name]\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for root, dir, files in os.walk(root_folder):\n",
    "        if root.split(os.sep)[-1] == class_name:\n",
    "            for i, file in enumerate(files):\n",
    "                if file.endswith('.JPEG'):\n",
    "                    images.append(plt.imread(os.path.join(root,file)))\n",
    "                if i+1 == N:\n",
    "                    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing class: pig\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7b3ad805deaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mshow_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'length'"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "class_name = 'pig'\n",
    "folder     = os.path.join('.', 'data', 'train')\n",
    "images     = get_N_images(N, class_name, folder)\n",
    "\n",
    "print(\"Visualizing class: {}\".format(class_name))\n",
    "for i in range(N):\n",
    "    show_image(images[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Normalize data\n",
    "\n",
    "Normalizing the data means getting the features in a similar range of values. This is an important step in your CNN pipeline. Making the features (here, pixels) in a similar data distribution helps the model converge.\n",
    "\n",
    "We've already looped through the training dataset and found the channel-wise means and standard deviations for you. We've divided them by 255, since the PyTorch Tensors have values between [0,1]. The images are normalized by subtracting the means and dividing by the standard deviations.  \n",
    "\n",
    "PyTorch has an easy method for integrating normalization into your machine learning pipeline - you create \n",
    "[transforms](https://pytorch.org/docs/master/torchvision/transforms.html?highlight=transform), which are just different data manipulations you can chain together. Before you pass an image to your model, it's fed through the transform first.\n",
    "\n",
    "The transform below converts the images into the correct format and then applies the normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_means = [123./255., 116./255.,  97./255.]\n",
    "dataset_stds  = [ 54./255.,  53./255.,  52./255.]\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(dataset_means, dataset_stds)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create Dataset and DataLoader for PyTorch model\n",
    "The model we will soon create in PyTorch needs a way of understanding our dataset folder structure.\n",
    "\n",
    "This has 2 steps:\n",
    "\n",
    "1. Define a **[Dataset](https://pytorch.org/docs/master/data.html#torch.utils.data.Dataset)**\n",
    "    - Tells the model where your data is and how to access it\n",
    "    - Requires `__getitem__` function, which tells your model how to grab an image and label when needed\n",
    "    - Requires `__len__` function, which returns the size of the dataset\n",
    "    \n",
    "   &nbsp; \n",
    "2. Define a **[DataLoader](https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader)**\n",
    "    - Tells the model how to sample from the dataset\n",
    "    - Defines the _batch size_, which is the number of images propagated through the network during one forward pass before a gradient update\n",
    "\n",
    "Run the cell below to create a class `AnimalDataset` which implements the required functions.\n",
    "Then, run the following cell to create a Dataset and DataLoader for both train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, root_dir, classes, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string):\n",
    "                Directory with all the images.\n",
    "                Of the form:\n",
    "                root_dir/\n",
    "                    <class_XX>/\n",
    "                        <XXXX>.JPEG\n",
    "                        ...\n",
    "                    <class_XX>/\n",
    "                        <XXXX>.JPEG\n",
    "                        ...\n",
    "                where <class_XX> is replaced with the class name, and <XXXX>.JPEG\n",
    "                are the images. Must be .JPEG extension.\n",
    "                \n",
    "            classes (list of strings): list of class names, same as names of\n",
    "                subfolders under root_dir\n",
    "                \n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "                \n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "\n",
    "        self.raw_data = []\n",
    "        for i, cl in enumerate(self.classes):\n",
    "            for root, directory, files in os.walk(os.path.join(root_dir, cl)):\n",
    "                for file in files:\n",
    "                    if '.JPEG' in file:\n",
    "                        self.raw_data.append((os.path.join(root, file), i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if type(idx) == torch.Tensor:\n",
    "            idx = idx.item()\n",
    "        \n",
    "        image_path, label = (self.raw_data[idx])\n",
    "        image = io.imread(image_path)\n",
    "        image = self.transform(image)\n",
    "        label = torch.tensor([label], dtype=torch.long)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataset = AnimalDataset(os.path.join('.', 'data', 'train'),classes, transform=transform)\n",
    "val_dataset   = AnimalDataset(os.path.join('.', 'data', 'val'),  classes, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color='red'>TODO [writeup] 1: </font>** \n",
    "Why is it important to have separate train and validation sets? What might happen if you _don't_ have a validation set?\n",
    "\n",
    "It is important to have separate train and validation sets because the model needs to be evaluated in some way, and so seeing how it does on a clean data set is essential. If the sets are not separate, then the validation set will be biased and the model will probably look better than it actually is. If you don't have a validation set at all, you will not be able to tell if your model does well on data it has never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Fitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-Level Overview\n",
    "Now, we will attempt to fit the data using a convolutional neural network. This network will learn to classify input images into one of the 16 animal categories, based on the training data that we provide to it. We will use our validation dataset to get a sense of how the network performs on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a baseline model\n",
    "Below we have defined a baseline model, which you will improve on in a later part of this project. There are two parts to defining a model:\n",
    "\n",
    "1. `__init__`: Define the **layers** of your network. Check out torch.nn documentation for pre-implemented layers.\n",
    "\n",
    "    &nbsp;\n",
    "2. `forward`: Chains together the layers you defined in `__init__`, creating the **pipeline** for a forward pass (i.e. where the image goes when you feed it to the network). In our baseline, the image passes through a series of **convolution** layers followed by **ReLU** (Rectified Linear Unit) nonlinearities, followed by a couple **fully connected** layers.\n",
    "\n",
    "### **<font color='red'>TODO [code]: </font>** \n",
    "\n",
    "Implement the baseline model architecture in `student.py`. The architecture is the following:\n",
    "- **conv1**: convolution layer with 6 output channels, kernel size of 3, stride of 2, padding of 1\n",
    "- **ReLU** nonlinearity\n",
    "- **conv2**: convolution layer with 12 output channels, kernel size of 3, stride of 2, padding of 1\n",
    "- **ReLU** nonlinearity\n",
    "- **conv3**: convolution layer with 24 output channels, kernel size of 3, stride of 2, padding of 1\n",
    "- **ReLU** nonlinearity\n",
    "- **fc**:    fully connected layer with 128 output features\n",
    "- **ReLU** nonlinearity\n",
    "- **cls**:   fully connected layer with 16 output features (the number of classes)\n",
    "\n",
    "\n",
    "\n",
    "The feature map sizes in this case are calculated by dividing the input size by the _stride_ (how many pixels you slide the kernel over each time you do a convolution). For example, we started with a 64 x 64 image, passed it through the `conv1` layer with a stride of 2, giving an output size of 32 x 32. When you create your own model, it's important to pay attention to these sizes. You'll need to figure out the dimensions to the first Linear layer, which needs an input size equal to the number of pixels in your Tensor by that point in the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that we added the correct layers above, we can **load the pretrained weights** into the architecture. The weights file essentially holds a dictionary, where the keys are the layer names and the values are the parameter weights. If your architecture is correct, you should be able to print the network and see the layers.\n",
    "\n",
    "**Please define the forward pass manually rather than using nn.Sequential. Otherwise, the pretrained model weights won't load correctly.**\n",
    "\n",
    "You'll have to create your own model, so make sure you understand the baseline architecture before proceeding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport student\n",
    "\n",
    "net_pretrained = student.AnimalBaselineNet()\n",
    "\n",
    "# Load pretrained weights into network to check if architecture is correct\n",
    "weights_path = os.path.join('.', 'models', 'baseline.pth')\n",
    "net_pretrained.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\n",
    "\n",
    "for layer in net_pretrained.children():\n",
    "    print(layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train the model\n",
    "\n",
    "Great, we have our model and our data! There's two more steps before we can train:\n",
    "- Decide what **loss function** you need. Here, we use [_cross entropy loss_](https://pytorch.org/docs/master/nn.html#crossentropyloss), typical for image classification.\n",
    "- Decide how many **epochs** you will train your model for. One epoch means one pass of the full training dataset through your model. Here, we set it to 30. When you train your own model, you'll have to keep track of the losses to decide when to stop.\n",
    "- Decide what **optimizer** you will use. An optimizer tells your model how to take steps along the gradient to try and reach a minimum. Here, we use the popular _Adam optimizer_ (if you're curious, here's the [paper](https://arxiv.org/pdf/1412.6980.pdf)). It adapts the learning rates for each parameter based on how quickly each parameter's gradient is changing. It's known as being more forgiving for less-than-optimal hyperparameter choices than other optimizers are.\n",
    "\n",
    "We also redefine the network to start from scratch, rather than loading in pretrained weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport student\n",
    "\n",
    "net = student.AnimalBaselineNet()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.01)\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train! For each epoch, the model does a forward and backward pass for training, and then just a forward pass for validation. But what are forward and backward passes?\n",
    "\n",
    "- **Forward pass**: sends a batch of images through the network. Returns the output of the last linear layer, which has 16 values: each value is the negative log-likelihood that the image belongs to that particular class.\n",
    "- **Backward pass**: calculates the loss and the gradient of the loss with respect to the model parameters. Optimizer updates the weights based on this gradient.\n",
    "\n",
    "During training, we keep track of the loss and accuracy for both training & validation phases so we can visualize our model performance after it's done.\n",
    "\n",
    "### **<font color='red'>TODO [code]: </font>** \n",
    "Implement the `model_train` function in `student.py`. We will be running your `model_train` with your `AnimalBaselineNet` to check accuracy.\n",
    "\n",
    "### **<font color='red'>TODO [writeup] 2a: </font>** \n",
    "Include the plots generated from training the baseline model from scratch (i.e., without loading in pretrained weights). What do you notice about the train vs. validation performance? What might this mean about the model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(curr_batch, batch_size, curr_epoch, total_epochs, dataloader):\n",
    "    \"\"\"\n",
    "    Prints message logging progress through training.\n",
    "    \"\"\"\n",
    "    progress = float(curr_batch + 1)/(float(len(dataloader.dataset)) / batch_size)\n",
    "    log = \"EPOCH [{}/{}].Progress: {} % \".format(\n",
    "        curr_epoch + 1, total_epochs, round(progress * 100, 2))\n",
    "    sys.stdout.write(\"\\r\" + log)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%aimport student\n",
    "\n",
    "# Keep track of average losses, training accuracy and validation accuracy for each epoch\n",
    "train_loss_history = np.zeros(epochs)\n",
    "train_acc_history  = np.zeros(epochs)\n",
    "val_loss_history   = np.zeros(epochs)\n",
    "val_acc_history    = np.zeros(epochs)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "     # ============================ Training ==============================\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    # Place network in training mode\n",
    "    net.train()\n",
    "    \n",
    "    # Initialize running epoch loss and number correctly classified\n",
    "    running_loss   = 0.0\n",
    "    num_correct    = 0.0\n",
    "    total_images   = 0.0\n",
    "    \n",
    "    \n",
    "    for batch_num, (inputs, labels) in enumerate(train_dataloader):\n",
    "        # [inputs] and [labels] is one batch of images and their classes\n",
    "\n",
    "        log_progress(batch_num, BATCH_SIZE, epoch, epochs, train_dataloader)\n",
    "        \n",
    "        # Function call to student\n",
    "        curr_loss, curr_correct, curr_images = \\\n",
    "            student.model_train(net, inputs, labels, criterion, optimizer)\n",
    "        running_loss += curr_loss\n",
    "        num_correct += curr_correct\n",
    "        total_images += curr_images\n",
    "\n",
    "    # Update statistics for epoch\n",
    "    train_loss_history[epoch] = running_loss / total_images\n",
    "    train_acc_history[epoch]  = float(num_correct)  / float(total_images)\n",
    "    print(\"\\n Train Avg. Loss: [{}] Acc: {} on {} images\\n\".format(\n",
    "          round(train_loss_history[epoch],4), train_acc_history[epoch], total_images) )\n",
    "    \n",
    "    # ============================ Validation ==============================\n",
    "    print(\"Validating...\")\n",
    "    # Place network in testing mode (won't need to keep track of gradients)\n",
    "    net.eval()\n",
    "    \n",
    "    running_loss   = 0.0\n",
    "    num_correct    = 0.0\n",
    "    total_images   = 0.0\n",
    "    \n",
    "    for batch_num, (inputs, labels) in enumerate(val_dataloader):\n",
    "        \n",
    "        # Propagate batch through network\n",
    "        outputs  = net(inputs)\n",
    "                                                 \n",
    "        # Calculate loss\n",
    "        loss     = criterion(outputs, labels.squeeze())\n",
    "                                                 \n",
    "        # Prediction is class with highest class score\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        running_loss  += loss.item()\n",
    "        num_correct   += torch.sum(preds == labels.data.reshape(-1))\n",
    "        total_images  += labels.data.numpy().size\n",
    "        \n",
    "    # Update statistics for validation data\n",
    "    val_loss_history[epoch] = running_loss / total_images\n",
    "    val_acc_history[epoch]  = float(num_correct)  / float(total_images) \n",
    "    print(\"Val Avg. Loss: [{}] Acc: {} on {} images\\n\".format(\n",
    "        round(val_loss_history[epoch],4), val_acc_history[epoch], total_images))\n",
    "    \n",
    "print(\"Time Elapsed: {} seconds\".format(\n",
    "    (datetime.now() - start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(11,5))\n",
    "\n",
    "ax = axs[0]\n",
    "ax.set_title(\"Average Losses\", fontsize=20)\n",
    "ax.plot(train_loss_history, label=\"Training\")\n",
    "ax.plot(val_loss_history,   label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\",        fontsize=16)\n",
    "ax.set_ylabel(\"Average loss\", fontsize=16)\n",
    "ax.legend(loc=\"best\",         fontsize=16)\n",
    "\n",
    "ax = axs[1]\n",
    "ax.set_title(\"Accuracy\", fontsize=22)\n",
    "ax.plot(train_acc_history, label=\"Training\")\n",
    "ax.plot(val_acc_history,   label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\",        fontsize=16)\n",
    "ax.set_ylabel(\"Accuracy\",     fontsize=16)\n",
    "ax.legend(loc=\"best\",         fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(transform):\n",
    "    \"\"\"\n",
    "    Returns dataloader for AnimalDataset given input transform, with batch size 1.\n",
    "    \"\"\"\n",
    "    dataset = AnimalDataset(os.path.join('.', 'data', 'val'),  classes, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "CNNs often require a large amount of image data in order to generalize well. A useful method for increasing the size of your dataset is called data augmentation.\n",
    "\n",
    "If you have an image of a sheep, you can shift it by a few pixels, rotate it, or change its contrast, and you'll still see an image of a sheep. So, you can **increase the size of your dataset** by applying small changes like these to your images while keeping the same labels.\n",
    "\n",
    "### <font color='red'>TODO [code]</font>:\n",
    "Implement the data augmentation transforms in `student.py`. You can visualize them using the cells below. When you're ready, you can save the resulting image to include in your writeup. Feel free to use anything in scipy, cv2, numpy, or skimage. However, **do not** use functions from torch or torchvision for this data augmentation step!\n",
    "\n",
    "### <font color='red'>TODO [writeup] 2b</font>:\n",
    "Include the plot from below with your images for data augmentation in the writeup. For a sample of correct behavior, look at the provided `data_augmentation_sample.png`. Note that you won't match it exactly since many values are randomized, but you can still see if your code is doing what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport student\n",
    "\n",
    "# Define display function to convert image to correct format\n",
    "to_pil = transforms.ToPILImage()\n",
    "convert = lambda image: to_pil(image.squeeze())\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15,10))\n",
    "\n",
    "ax = axs[0,0]\n",
    "ax.set_title(\"Original\")\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "img, label = next(iter(get_dataloader(transform)))\n",
    "ax.imshow(convert(img))\n",
    "\n",
    "ax = axs[0,1]\n",
    "ax.set_title(\"Constrast\")\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            student.Contrast(min_contrast=0.3, max_contrast=0.9)\n",
    "        ])\n",
    "img, label = next(iter(get_dataloader(transform)))\n",
    "ax.imshow(convert(img))\n",
    "\n",
    "ax = axs[0,2]\n",
    "ax.set_title(\"Shift\")\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            student.Shift(max_shift=5)\n",
    "        ])\n",
    "img, label = next(iter(get_dataloader(transform)))\n",
    "ax.imshow(convert(img))\n",
    "\n",
    "ax = axs[1,0]\n",
    "ax.set_title(\"Rotate\")\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            student.Rotate(max_angle=10)\n",
    "        ])\n",
    "img, label = next(iter(get_dataloader(transform)))\n",
    "ax.imshow(convert(img))\n",
    "\n",
    "ax = axs[1,1]\n",
    "ax.set_title(\"Flip\")\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            student.HorizontalFlip(p=1)\n",
    "        ])\n",
    "img, label = next(iter(get_dataloader(transform)))\n",
    "ax.imshow(convert(img))\n",
    "\n",
    "\n",
    "ax = axs[1,2]\n",
    "ax.set_title(\"Combination\")\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            student.Contrast(min_contrast=0.3, max_contrast=0.9),\n",
    "            student.Shift(max_shift=5),\n",
    "            student.Rotate(max_angle=10),\n",
    "            student.HorizontalFlip(p=0.5),\n",
    "        ])\n",
    "img, label = next(iter(get_dataloader(transform)))\n",
    "ax.imshow(convert(img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Building a better model\n",
    "\n",
    "Now, it's your turn to build a model. You'll submit your predictions to the Kaggle competition. Your goal: beat the instructor model, while keeping model file size below 5 MB (don't worry, the instructor model follows this restriction too). A few things you can try:\n",
    "- include data augmentation methods in your transform\n",
    "- add pooling\n",
    "- adjust hyperparameters (e.g. batch size, learning rate)\n",
    "- adjust number of epochs\n",
    "\n",
    "\n",
    "### <font color='red'>TODO [code]</font>:\n",
    "In `student.py`, define your own `AnimalStudentNet` model architecture and transform pipeline. We will be running your `model_train` with your `AnimalStudentNet` as well as `get_student_settings` when grading to check consistency with what you submit to Kaggle. Please make sure you follow the following restrictions when building your network. \n",
    "#### Restrictions ####\n",
    "* Model must be below 5 MB\n",
    "* You may not use pretrained models or model architectures from the internet. We are expecting you to build one from scratch. That being said, you may take inspirtation from model architectures found online or in research but you must implement them from scratch and explain your decisions. \n",
    "* You may not use any other dataset than the one provided in the assignment. \n",
    "* You may not use data augmentation functions built into torch. Feel free to use your own or define new ones. \n",
    "* You may not hand-label the test set. \n",
    "* You may not share models between groups. \n",
    "\n",
    "### <font color='red'>TODO [writeup] 3</font>:\n",
    "Include the plots showing your model's loss and accuracy history. Discuss your decisions for your model architecture, as well as `get_student_settings` if you changed the parameters from the baseline. What worked and what didn't? What might be some reasons why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport student\n",
    "\n",
    "net = student.AnimalStudentNet()\n",
    "transform, batch_size, epochs, criterion, optimizer = student.get_student_settings(net)\n",
    "\n",
    "for layer in net.children():\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AnimalDataset(os.path.join('.', 'data', 'train'),classes, transform=transform)\n",
    "val_dataset   = AnimalDataset(os.path.join('.', 'data', 'val'),  classes, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%aimport student\n",
    "\n",
    "# Keep track of average losses, training accuracy and validation accuracy for each epoch\n",
    "train_loss_history = np.zeros(epochs)\n",
    "train_acc_history  = np.zeros(epochs)\n",
    "val_loss_history   = np.zeros(epochs)\n",
    "val_acc_history    = np.zeros(epochs)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "     # ============================ Training ==============================\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    # Place network in training mode\n",
    "    net.train()\n",
    "    \n",
    "    # Initialize running epoch loss and number correctly classified\n",
    "    running_loss   = 0.0\n",
    "    num_correct    = 0.0\n",
    "    total_images   = 0.0\n",
    "    \n",
    "    for batch_num, (inputs, labels) in enumerate(train_dataloader):\n",
    "        # [inputs] and [labels] is one batch of images and their classes\n",
    "\n",
    "        log_progress(batch_num, batch_size, epoch, epochs, train_dataloader)\n",
    "        \n",
    "        #  ***** Function call to student *****\n",
    "        curr_loss, curr_correct, curr_images = \\\n",
    "            student.model_train(net, inputs, labels, criterion, optimizer)\n",
    "        running_loss += curr_loss\n",
    "        num_correct += curr_correct\n",
    "        total_images += curr_images\n",
    "\n",
    "    # Update statistics for epoch\n",
    "    train_loss_history[epoch] = running_loss / total_images\n",
    "    train_acc_history[epoch]  = float(num_correct)  / float(total_images)\n",
    "    print(\"\\n Train Avg. Loss: [{}] Acc: {} on {} images\\n\".format(\n",
    "          round(train_loss_history[epoch],4), train_acc_history[epoch], total_images) )\n",
    "    \n",
    "    # ============================ Validation ==============================\n",
    "    print(\"Validating...\")\n",
    "    # Place network in testing mode (won't need to keep track of gradients)\n",
    "    net.eval()\n",
    "    \n",
    "    running_loss   = 0.0\n",
    "    num_correct    = 0.0\n",
    "    total_images   = 0.0\n",
    "    \n",
    "    for batch_num, (inputs, labels) in enumerate(val_dataloader):\n",
    "        \n",
    "        # Propagate batch through network\n",
    "        outputs  = net(inputs)\n",
    "                                                 \n",
    "        # Calculate loss\n",
    "        loss     = criterion(outputs, labels.squeeze())\n",
    "                                                 \n",
    "        # Prediction is class with highest class score\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        running_loss  += loss.item()\n",
    "        num_correct   += torch.sum(preds == labels.data.reshape(-1))\n",
    "        total_images  += labels.data.numpy().size\n",
    "        \n",
    "    # Update statistics for validation data\n",
    "    val_loss_history[epoch] = running_loss / total_images\n",
    "    val_acc_history[epoch]  = float(num_correct)  / float(total_images) \n",
    "    print(\"Val Avg. Loss: [{}] Acc: {} on {} images\\n\".format(\n",
    "        round(val_loss_history[epoch],4), val_acc_history[epoch], total_images))\n",
    "    \n",
    "print(\"Time Elapsed: {} seconds\".format(\n",
    "    (datetime.now() - start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(11,5))\n",
    "\n",
    "ax = axs[0]\n",
    "ax.set_title(\"Student Avg Losses\", fontsize=20)\n",
    "ax.plot(train_loss_history, label=\"Training\")\n",
    "ax.plot(val_loss_history,   label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\",        fontsize=16)\n",
    "ax.set_ylabel(\"Average loss\", fontsize=16)\n",
    "ax.legend(loc=\"best\",         fontsize=16)\n",
    "\n",
    "ax = axs[1]\n",
    "ax.set_title(\"Student Accuracy\", fontsize=22)\n",
    "ax.plot(train_acc_history, label=\"Training\")\n",
    "ax.plot(val_acc_history,   label=\"Validation\")\n",
    "ax.set_xlabel(\"Epoch\",        fontsize=16)\n",
    "ax.set_ylabel(\"Accuracy\",     fontsize=16)\n",
    "ax.legend(loc=\"best\",         fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the Model####\n",
    "If you would like to save your model after you have trained it for later use (like in the kaggle notebook). Please use torch.save as below. This will save your model as 'my_model_name.pth' in the models folder. Feel free to modify the name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), os.path.join('.','models','my_model_name.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Adversarial Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks are extremely powerful models when it comes to images; today, they are used extensively in drug discovery, disease detection, self-driving cars, and more. However, they are far from perfect. Turns out, these networks can be tricked very easily, using the concept of _adversarial examples_. In this part, we will generate adversarial examples to trick our own model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-Level Overview\n",
    "We start with an image, $I$, that is classified correctly as class $C$. Our objective is to **manipulate the image** $I$ by adding **small changes** to it which would make the network **misclassify** the image. We can do so by observing the gradients produced within the model. Essentially, we find the direction in which we must change each input image pixel to maximize the loss, and we give the image's pixels a slight nudge in that direction. We get a new image, $I_{perturbed}$, that the network misclassifies.\n",
    "\n",
    "The scary part is that $I_{perturbed}$ contains tiny, imperceptible changes relative to $I$; to the human eye, $I$ and $I_{perturbed}$ visually look like the same image. Yet, the model perceives these two images completely differently.\n",
    "\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Below is a loop that loads the baseline model and sends images through it one by one. When it classifies an image correctly, you will create an adversarial image and see if you can fool the network into misclassifying it.\n",
    "\n",
    "\n",
    "For each correctly classified image, we need the gradient of the loss with respect to the image: $G = \\frac{\\partial_{loss}}{\\partial_{I}}$. We use this to create a matrix $\\alpha$ (defined below) of perturbations and add it to the original image:\n",
    "\n",
    "$$I_{perturbed} = I + \\alpha$$\n",
    "\n",
    "$\\alpha$ and $G$ have the same dimensions. $\\alpha$ is a matrix where the absolute value of each element is a small $\\epsilon$, yet the _sign_ (positive or negative) of $\\alpha[i,j]$ is equal to the _sign_ of $G[i,j]$. This makes sense: if the gradient value at some pixel is  negative, then you want to follow that negative slope and add negative noise value to your original image.\n",
    "\n",
    "\n",
    "### **<font color=\"red\">TODO [code]: </font>**\n",
    "Complete the `get_adversarial` in `student.py`. Run the cells below. Your function should get **at least 85% of adversarial images misclassified** with an `epsilon=0.02`.\n",
    "\n",
    "### **<font color=\"red\">TODO [writeup] 4: </font>**\n",
    "Include 3 adversarial image plots from the cells below in your writeup: one with `epsilon=0.005`, one with `epsilon=0.02`, and one with `epsilon=0.15`. As you increase epsilon, what happens to the misclassification rate? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport student\n",
    "\n",
    "# Load pretrained baseline model\n",
    "net          = student.AnimalBaselineNet()\n",
    "weights_path = os.path.join('.', 'models', 'baseline.pth')\n",
    "net.load_state_dict(torch.load(weights_path, map_location=\"cpu\"))\n",
    "\n",
    "# Specify parameters for forward pass\n",
    "# Don't normalize images for purpose of visualization\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "val_dataset    = AnimalDataset(os.path.join('.', 'data', 'val'),  classes, transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epsilon = 0.02\n",
    "\n",
    "original_imgs      = []\n",
    "original_preds     = []\n",
    "\n",
    "adversarial_imgs   = []\n",
    "adversarial_preds  = []\n",
    "adversarial_noises = []\n",
    "\n",
    "\n",
    "for i, (img, label) in enumerate(val_dataloader):\n",
    "    \n",
    "    # Set image tensor so gradient is calculated\n",
    "    img.requires_grad = True\n",
    "    \n",
    "    output = net(img)\n",
    "\n",
    "    init_pred = output.max(1, keepdim=True)[1]\n",
    "    \n",
    "    if init_pred == label:\n",
    "        # Image classified correctly; generate adversarial image\n",
    "        perturbed_img, noise = student.get_adversarial(img, output, label[0], net, criterion, epsilon)\n",
    "    \n",
    "        adversarial_output = net(perturbed_img)\n",
    "        adversarial_pred   = adversarial_output.max(1, keepdim=True)[1]\n",
    "\n",
    "        original_imgs.append(to_pil(img.squeeze()))\n",
    "        original_preds.append(init_pred)\n",
    "        adversarial_imgs.append(to_pil(perturbed_img.squeeze()))\n",
    "        adversarial_preds.append(adversarial_pred)\n",
    "        adversarial_noises.append(to_pil(noise.squeeze()))\n",
    "    \n",
    "\n",
    "\n",
    "print(\"Out of total {} images generated, {} % of adversarial images misclassified\".format(\n",
    "    len(original_imgs),\n",
    "    round((torch.sum(torch.Tensor(original_preds) != torch.Tensor(adversarial_preds)).item())\n",
    "        / len(original_imgs) * 100., 4)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_desired_adversarial = 4\n",
    "\n",
    "fig, axs = plt.subplots(num_desired_adversarial, 3, figsize=(10,3*num_desired_adversarial))\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "\n",
    "\n",
    "for i in range(num_desired_adversarial):\n",
    "    ax = axs[i,0]\n",
    "    ax.set_title('Original: {}'.format(classes[original_preds[i]]))\n",
    "    ax.imshow(original_imgs[i])\n",
    "    \n",
    "    ax = axs[i,1]\n",
    "    ax.set_title('Adversarial Noise')\n",
    "    ax.imshow(adversarial_noises[i])\n",
    "    \n",
    "    ax = axs[i,2]\n",
    "    ax.set_title('Adversarial: {}'.format(classes[adversarial_preds[i]]))\n",
    "    ax.imshow(adversarial_imgs[i])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### On a positive note...\n",
    "While this may seem like a dealbreaker for convolutional neural networks, a lot of research is being done on the topic of adversarial examples. People have been finding ways to make our networks more robust to adversarial attacks, and there's also a whole class of networks called Generative Adversarial Networks (GANs) that rely on the concept of adversarial actors to generate new, synthetic data for a given dataset.\n",
    "\n",
    "If you're interested in learning more about any of the topics in this project, please post on Piazza or ask in office hours and we'd be happy to share more resources :)\n",
    "\n",
    "\n",
    "### **<font color=\"red\">TODO [writeup] Feedback (not graded, but very appreciated): </font>**\n",
    "If you have any feedback on this assignment as a whole, please include! It is a new assignment, so we would love to hear your thoughts on it and how we can make it better next time around. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
